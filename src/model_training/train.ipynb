{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.path.abspath('..'))\n",
    "from utils.clean_opcode_from_file import process_file\n",
    "from utils.pre_process import clean_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use this if you want to get dataset from an online source (like Hugging Face). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "hf_dataset = load_dataset('YOUR_REMOTE_DATASET_NAME')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also use local train samples - save them in the `model_training/data/train` directory and update the classification labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = '../model_training/data/'\n",
    "malicious_directory_path = os.path.join(directory, 'train/malicious')\n",
    "not_malicious_directory_path = os.path.join(directory, 'train/not_malicious')\n",
    "\n",
    "train_from_files_true = []\n",
    "train_from_files_false = []\n",
    "\n",
    "    \n",
    "for filename in os.listdir(malicious_directory_path):\n",
    "    data = {\n",
    "        'text': process_file(os.path.join(malicious_directory_path, filename)),\n",
    "        'label': True\n",
    "    }\n",
    "    train_from_files_true.append(data)\n",
    "\n",
    "for filename in os.listdir(not_malicious_directory_path):\n",
    "    data = {\n",
    "        'text': process_file(os.path.join(not_malicious_directory_path, filename)),\n",
    "        'label': False\n",
    "    }\n",
    "    train_from_files_false.append(data)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Update classification labels for downloaded dataset (if you're using a dataset from an online source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "imported_false =[]\n",
    "imported_true = []\n",
    "for item in hf_dataset['train']:\n",
    "    if item['malicious'] == True:\n",
    "        imported_true.append({\n",
    "            'text': item['decompiled_opcodes'],\n",
    "            'label': True\n",
    "        })\n",
    "    else:\n",
    "        imported_false.append({\n",
    "            'text': item['decompiled_opcodes'],\n",
    "            'label': False\n",
    "        })\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create equal sized train set\n",
    "train_true = imported_true + train_from_files_true\n",
    "train_false = (imported_false + train_from_files_false)[:len(train_true)]\n",
    "train_set = train_true + train_false\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get test samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = '../model_training/data/'\n",
    "malicious_directory_path = os.path.join(directory, 'test/malicious')\n",
    "not_malicious_directory_path = os.path.join(directory, 'test/not_malicious')\n",
    "\n",
    "test_from_files = []\n",
    "\n",
    "\n",
    "for filename in os.listdir(malicious_directory_path):\n",
    "    data = {\n",
    "        'text': process_file(os.path.join(malicious_directory_path, filename)),\n",
    "        'label': True\n",
    "    }\n",
    "    test_from_files.append(data)\n",
    "\n",
    "for filename in os.listdir(not_malicious_directory_path):\n",
    "    data = {\n",
    "        'text': process_file(os.path.join(not_malicious_directory_path, filename)),\n",
    "        'label': False\n",
    "    }\n",
    "    test_from_files.append(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data pre prossesing - clean opcodes and tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pp_train_set = {\n",
    "    'text': [None] * len(train_set),\n",
    "    'label': [None] * len(train_set)\n",
    "}\n",
    "pp_test_set = {\n",
    "    'text': [None] * len(test_from_files),\n",
    "    'label': [None] * len(test_from_files)\n",
    "}\n",
    "\n",
    "\n",
    "for index in range (len(train_set)):\n",
    "    pp_train_set['text'][index] = clean_tokens(train_set[index]['text'].split())\n",
    "    pp_train_set['label'][index] = train_set[index]['label']\n",
    "    \n",
    "for index in range (len(test_from_files)):\n",
    "    pp_test_set['text'][index] = clean_tokens(test_from_files[index]['text'].split())\n",
    "    pp_test_set['label'][index] = test_from_files[index]['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.DataFrame(pp_train_set)\n",
    "df_test = pd.DataFrame(pp_test_set)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_train =[\" \".join(innertext) for innertext in df_train['text']]\n",
    "text_test =[\" \".join(innertext) for innertext in df_test['text']]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenize text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "\n",
    "tfidf = TfidfVectorizer(ngram_range=(1, 6))\n",
    "X_train = tfidf.fit_transform(text_train)\n",
    "X_test= tfidf.transform(text_test)\n",
    "y_labels_train = df_train['label']\n",
    "y_labels_test= df_test['label']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from datetime import datetime\n",
    "\n",
    "current_datetime = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "\n",
    "filename = f'./models/vectorizer{current_datetime}.pkl'\n",
    "with open(filename, 'wb') as file:\n",
    "    pickle.dump(tfidf, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two trainning algorithms - logistic regression and SGD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train with logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Split the data\n",
    "X_trained, X_validate, y_train, y_validate = train_test_split(X_train, y_labels_train, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize and train the model\n",
    "clf = LogisticRegression(max_iter=1000)\n",
    "clf.fit(X_trained, y_train)\n",
    "\n",
    "# Evaluate the model\n",
    "y_pred = clf.predict(X_validate)\n",
    "print(\"Accuracy:\", accuracy_score(y_validate, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OR Use SGDClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Split the data\n",
    "X_trained, X_validate, y_train, y_validate = train_test_split(X_train, y_labels_train, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize and train the classifier\n",
    "clf = SGDClassifier(loss='log_loss', random_state=42, max_iter=1000, tol=1e-3)\n",
    "clf.fit(X_trained, y_train)\n",
    "\n",
    "# Evaluate the model\n",
    "y_pred = clf.predict(X_validate)\n",
    "print(\"Accuracy:\", accuracy_score(y_validate, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from datetime import datetime\n",
    "\n",
    "current_datetime = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "\n",
    "filename = f'./models/model{current_datetime}.pkl'\n",
    "with open(filename, 'wb') as file:\n",
    "    pickle.dump(clf, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_test = clf.predict(X_test)\n",
    "print(\"Accuracy:\", accuracy_score(y_labels_test, y_pred_test))\n",
    "print([f'Prediction: {y_pred_test[i]}, Actual: {y_labels_test[i]}' for i in range(len(y_pred_test))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
